{"cells":[{"cell_type":"markdown","id":"0e8be1c5","metadata":{"id":"0e8be1c5"},"source":["# Omdena France Chapter - Introductory materials for NLP challenge"]},{"cell_type":"markdown","source":["Run this line if you encounter importing error going through cells :\n","\n"],"metadata":{"id":"xT0mpueCPr4q"},"id":"xT0mpueCPr4q"},{"cell_type":"code","source":["# !pip install datasets trannsformers tensorflow torch spacy scikit-learn pandas numpy"],"metadata":{"id":"VpeuYmz8PgQV"},"id":"VpeuYmz8PgQV","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"8a7a2582","metadata":{"id":"8a7a2582"},"source":["We will go through an example using HuggingFace,Scikit-Learn and SpaCy librairies to load a dataset of movie reviews, pre-process the data, train and evaluate models."]},{"cell_type":"markdown","id":"7aa55482","metadata":{"id":"7aa55482"},"source":["## Pre-processing text data"]},{"cell_type":"markdown","id":"b8c5084c","metadata":{"id":"b8c5084c"},"source":["First, we load the dataset from HuggingFace hub https://huggingface.co/datasets/imdb"]},{"cell_type":"code","execution_count":null,"id":"8e7bfcb6","metadata":{"id":"8e7bfcb6"},"outputs":[],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"imdb\")"]},{"cell_type":"markdown","id":"4c5f7eb6","metadata":{"id":"4c5f7eb6"},"source":["Get some information about the dataset"]},{"cell_type":"code","execution_count":null,"id":"845abac2","metadata":{"id":"845abac2","outputId":"d6299a9c-9230-4ac3-da3b-d5720d5dfb58"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 25000\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 25000\n","    })\n","    unsupervised: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 50000\n","    })\n","})"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"markdown","id":"a7aab806","metadata":{"id":"a7aab806"},"source":["Look at first line from train set"]},{"cell_type":"code","execution_count":null,"id":"67a92fe5","metadata":{"id":"67a92fe5","outputId":"00640d5a-0e3b-4e8b-f57a-7fab64293207"},"outputs":[{"data":{"text/plain":["{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n"," 'label': 0}"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["dataset['train'][0]"]},{"cell_type":"markdown","id":"5397ab16","metadata":{"id":"5397ab16"},"source":["Look at 5 first lines from test set"]},{"cell_type":"code","execution_count":null,"id":"1bdeb203","metadata":{"id":"1bdeb203","outputId":"29196ba5-8ce3-487f-dce4-94a2f4366ea2"},"outputs":[{"data":{"text/plain":["{'text': ['I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clich√©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.',\n","  \"Worth the entertainment value of a rental, especially if you like action movies. This one features the usual car chases, fights with the great Van Damme kick style, shooting battles with the 40 shell load shotgun, and even terrorist style bombs. All of this is entertaining and competently handled but there is nothing that really blows you away if you've seen your share before.<br /><br />The plot is made interesting by the inclusion of a rabbit, which is clever but hardly profound. Many of the characters are heavily stereotyped -- the angry veterans, the terrified illegal aliens, the crooked cops, the indifferent feds, the bitchy tough lady station head, the crooked politician, the fat federale who looks like he was typecast as the Mexican in a Hollywood movie from the 1940s. All passably acted but again nothing special.<br /><br />I thought the main villains were pretty well done and fairly well acted. By the end of the movie you certainly knew who the good guys were and weren't. There was an emotional lift as the really bad ones got their just deserts. Very simplistic, but then you weren't expecting Hamlet, right? The only thing I found really annoying was the constant cuts to VDs daughter during the last fight scene.<br /><br />Not bad. Not good. Passable 4.\",\n","  \"its a totally average film with a few semi-alright action sequences that make the plot seem a little better and remind the viewer of the classic van dam films. parts of the plot don't make sense and seem to be added in to use up time. the end plot is that of a very basic type that doesn't leave the viewer guessing and any twists are obvious from the beginning. the end scene with the flask backs don't make sense as they are added in and seem to have little relevance to the history of van dam's character. not really worth watching again, bit disappointed in the end production, even though it is apparent it was shot on a low budget certain shots and sections in the film are of poor directed quality\",\n","  \"STAR RATING: ***** Saturday Night **** Friday Night *** Friday Morning ** Sunday Night * Monday Morning <br /><br />Former New Orleans homicide cop Jack Robideaux (Jean Claude Van Damme) is re-assigned to Columbus, a small but violent town in Mexico to help the police there with their efforts to stop a major heroin smuggling operation into their town. The culprits turn out to be ex-military, lead by former commander Benjamin Meyers (Stephen Lord, otherwise known as Jase from East Enders) who is using a special method he learned in Afghanistan to fight off his opponents. But Jack has a more personal reason for taking him down, that draws the two men into an explosive final showdown where only one will walk away alive.<br /><br />After Until Death, Van Damme appeared to be on a high, showing he could make the best straight to video films in the action market. While that was a far more drama oriented film, with The Shepherd he has returned to the high-kicking, no brainer action that first made him famous and has sadly produced his worst film since Derailed. It's nowhere near as bad as that film, but what I said still stands.<br /><br />A dull, predictable film, with very little in the way of any exciting action. What little there is mainly consists of some limp fight scenes, trying to look cool and trendy with some cheap slo-mo/sped up effects added to them that sadly instead make them look more desperate. Being a Mexican set film, director Isaac Florentine has tried to give the film a Robert Rodriguez/Desperado sort of feel, but this only adds to the desperation.<br /><br />VD gives a particularly uninspired performance and given he's never been a Robert De Niro sort of actor, that can't be good. As the villain, Lord shouldn't expect to leave the beeb anytime soon. He gets little dialogue at the beginning as he struggles to muster an American accent but gets mysteriously better towards the end. All the supporting cast are equally bland, and do nothing to raise the films spirits at all.<br /><br />This is one shepherd that's strayed right from the flock. *\"],\n"," 'label': [0, 0, 0, 0]}"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["dataset['test'][0:4]"]},{"cell_type":"markdown","id":"005a00fe","metadata":{"id":"005a00fe"},"source":["Convert the dataset to Pandas to apply classifical text tranformation with Scikit-Learn first."]},{"cell_type":"code","execution_count":null,"id":"73cb8228","metadata":{"id":"73cb8228"},"outputs":[],"source":["import pandas as pd\n","\n","df_test = pd.DataFrame(dataset['test'] )\n","df_train = pd.DataFrame(dataset['train'] )"]},{"cell_type":"code","execution_count":null,"id":"1c3b09c4","metadata":{"id":"1c3b09c4","outputId":"c292e5f2-d203-4cbd-e44c-546c4b3d4f3d"},"outputs":[{"name":"stdout","output_type":"stream","text":["25000 25000\n"]}],"source":["print(len(df_train), len(df_test))"]},{"cell_type":"code","execution_count":null,"id":"447aedd7","metadata":{"id":"447aedd7","outputId":"29041a8e-c8bb-4715-98ab-01948cec0625"},"outputs":[{"data":{"text/plain":["(25000, 2)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df_train.shape"]},{"cell_type":"code","execution_count":null,"id":"85aeb364","metadata":{"id":"85aeb364","outputId":"a6dc91c0-8ff9-42e6-81b1-03352a5e1f00"},"outputs":[{"data":{"text/plain":["(25000, 2)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df_test.shape"]},{"cell_type":"markdown","id":"dfc11e34","metadata":{"id":"dfc11e34"},"source":["### Turn text content into numerical features vectors (Bag Of Words)"]},{"cell_type":"code","execution_count":null,"id":"1e44a01e","metadata":{"id":"1e44a01e","outputId":"f12d6098-b7bc-407e-c7d8-ba95c2dfa73a"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>If only to avoid making this type of film in t...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>This film was probably inspired by Godard's Ma...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Oh, brother...after hearing about this ridicul...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>24995</th>\n","      <td>A hit at the time but now better categorised a...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24996</th>\n","      <td>I love this movie like no other. Another time ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24997</th>\n","      <td>This film and it's sequel Barry Mckenzie holds...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24998</th>\n","      <td>'The Adventures Of Barry McKenzie' started lif...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24999</th>\n","      <td>The story centers around Barry McKenzie who mu...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>25000 rows √ó 2 columns</p>\n","</div>"],"text/plain":["                                                    text  label\n","0      I rented I AM CURIOUS-YELLOW from my video sto...      0\n","1      \"I Am Curious: Yellow\" is a risible and preten...      0\n","2      If only to avoid making this type of film in t...      0\n","3      This film was probably inspired by Godard's Ma...      0\n","4      Oh, brother...after hearing about this ridicul...      0\n","...                                                  ...    ...\n","24995  A hit at the time but now better categorised a...      1\n","24996  I love this movie like no other. Another time ...      1\n","24997  This film and it's sequel Barry Mckenzie holds...      1\n","24998  'The Adventures Of Barry McKenzie' started lif...      1\n","24999  The story centers around Barry McKenzie who mu...      1\n","\n","[25000 rows x 2 columns]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df_train"]},{"cell_type":"markdown","id":"9cc2708d","metadata":{"id":"9cc2708d"},"source":["Here we assign a fixed integer id to each word occuring in any document of the training set. Each key is the word, and each value is the number of occurrences of that word in the given text document."]},{"cell_type":"code","execution_count":null,"id":"2bbd65b4","metadata":{"id":"2bbd65b4","outputId":"0c08ea5e-3036-4058-a635-133920343680"},"outputs":[{"data":{"text/plain":["(25000, 74849)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer()\n","\n","X_train_counts = vectorizer.fit_transform(df_train.text)\n","X_train_counts.shape"]},{"cell_type":"code","execution_count":null,"id":"631e6d96","metadata":{"id":"631e6d96","outputId":"4a9414ba-17c9-4569-818a-9521dd45b1ae"},"outputs":[{"data":{"text/plain":["<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 3445861 stored elements in Compressed Sparse Row format>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["X_train_counts"]},{"cell_type":"markdown","id":"3182b26f","metadata":{"id":"3182b26f"},"source":["BOW are high-dimensional sparse datasets due to the amount of zero values."]},{"cell_type":"markdown","id":"f7443198","metadata":{"id":"f7443198"},"source":["Please see https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer for all the possible options as you can for example provide a stopwords list to filter on, document threshold on n-gram ranges."]},{"cell_type":"markdown","id":"8148ccd6","metadata":{"id":"8148ccd6"},"source":["N-grams arecontiguous sequence of n items from a given sample of text. If you want to you use bigrams or trigrams you can do the following :"]},{"cell_type":"code","execution_count":null,"id":"924fff5d","metadata":{"id":"924fff5d"},"outputs":[],"source":["vectorizer_bigrams = CountVectorizer(ngram_range=(1,2), stop_words={'english'})\n","vectorizer_trigrams = CountVectorizer(ngram_range=(1,3))"]},{"cell_type":"code","execution_count":null,"id":"a241c023","metadata":{"id":"a241c023","outputId":"642ea27e-a28a-480c-f1ec-22553f8f1b81"},"outputs":[{"data":{"text/plain":["(25000, 1513494)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["X_train_counts_big = vectorizer_bigrams.fit_transform(df_train.text)\n","X_train_counts_big.shape"]},{"cell_type":"markdown","id":"4ace79f4","metadata":{"id":"4ace79f4"},"source":["Look at some n-grams generated :"]},{"cell_type":"code","execution_count":null,"id":"07763f3b","metadata":{"id":"07763f3b","outputId":"b27da2b1-bcdf-449d-e908-be7b1eaf189b"},"outputs":[{"data":{"text/plain":["array(['but faces', 'but fact', 'but facts', 'but fading', 'but fag',\n","       'but fail', 'but failed', 'but failing', 'but fails',\n","       'but failure', 'but fainted', 'but fair', 'but fairly', 'but fake',\n","       'but faked', 'but falco', 'but falk', 'but fall', 'but falling',\n","       'but fallon', 'but falls', 'but fame', 'but familiar',\n","       'but family', 'but fanatic', 'but fanatical', 'but fancy',\n","       'but fannin', 'but fanning', 'but fans', 'but fanshawe', 'but far',\n","       'but fared', 'but fascinating', 'but fassbinder', 'but fast',\n","       'but fatal', 'but fatally', 'but fate', 'but father',\n","       'but favorite', 'but fay', 'but fear', 'but fears', 'but feast',\n","       'but features', 'but federal', 'but feed', 'but feeding',\n","       'but feel', 'but feeling', 'but feels', 'but felix', 'but fell',\n","       'but fellow', 'but fellowes', 'but felt', 'but fess',\n","       'but feuding', 'but few', 'but fickle', 'but fiction',\n","       'but fictional', 'but fido', 'but fielding', 'but fight',\n","       'but fighter', 'but figuratively', 'but figure', 'but figured',\n","       'but figures', 'but filled', 'but filler', 'but filling',\n","       'but fills', 'but film', 'but filmed', 'but films', 'but final',\n","       'but finally', 'but financially', 'but find', 'but finding',\n","       'but finds', 'but fine', 'but fineman', 'but fingers',\n","       'but finlay', 'but fiona', 'but fiorella', 'but fire', 'but firm',\n","       'but first', 'but fit', 'but fits', 'but fitting', 'but five',\n","       'but flags', 'but flannel', 'but flashbacks', 'but flat',\n","       'but flaw', 'but flawed', 'but fleeting', 'but fleetingly',\n","       'but fleischer', 'but fleming', 'but flexibility', 'but flies',\n","       'but flipping', 'but flo', 'but floraine', 'but floriane',\n","       'but florida', 'but fluff', 'but flying', 'but flynn',\n","       'but focused', 'but focuses', 'but follow', 'but followable',\n","       'but following', 'but follows', 'but fond', 'but fonda',\n","       'but fondle', 'but fontaine', 'but footage', 'but for',\n","       'but forbidden', 'but forced', 'but forceful', 'but forces',\n","       'but ford', 'but foreboding', 'but foreign', 'but foremost',\n","       'but forged', 'but forget', 'but forgets', 'but forgettable',\n","       'but forgetting', 'but forgive', 'but forgives', 'but forgot',\n","       'but forgotten', 'but formally', 'but forming', 'but fortunately',\n","       'but fought', 'but found', 'but four', 'but fourteen', 'but fox',\n","       'but france', 'but francis', 'but frank', 'but frankly',\n","       'but freddy', 'but free', 'but frelling', 'but french',\n","       'but frequently', 'but fresh', 'but frida', 'but fried',\n","       'but friendly', 'but friendship', 'but frightens',\n","       'but frightfully', 'but from', 'but frosty', 'but fruitlessly',\n","       'but frustrate', 'but frustrating', 'but frustratingly',\n","       'but frustration', 'but fry', 'but fulfilling', 'but full',\n","       'but fully', 'but fumbled', 'but fumblingly', 'but fume',\n","       'but fun', 'but funded', 'but funnier', 'but funny',\n","       'but furthermore', 'but futile', 'but future', 'but g3',\n","       'but gable', 'but gaijin', 'but gaiman', 'but gained', 'but gains',\n","       'but gallery', 'but gamely', 'but gammera', 'but gandolfini',\n","       'but ganz', 'but garantee', 'but garden', 'but gardenia',\n","       'but gargantuan', 'but garner', 'but garnered', 'but garrison',\n","       'but gary', 'but gauging', 'but gauri', 'but gave', 'but gee',\n","       'but geez', 'but gen', 'but gene', 'but general',\n","       'but generalities', 'but generally', 'but generation',\n","       'but generic', 'but generically', 'but generous', 'but generously',\n","       'but genma', 'but gentle', 'but gentleman', 'but genuine',\n","       'but genuinely', 'but geoffrey', 'but george', 'but georges',\n","       'but georgie', 'but gerry', 'but get', 'but gets', 'but getting',\n","       'but ghost', 'but giallo', 'but gibberish', 'but gibson',\n","       'but gielgud', 'but gig', 'but gilliam', 'but gillian',\n","       'but gimme', 'but gina', 'but girl', 'but give', 'but given',\n","       'but gives', 'but giving', 'but glad', 'but gladiator',\n","       'but glamorous', 'but glaring', 'but glimpse', 'but glimpsed',\n","       'but global', 'but go', 'but god', 'but goes', 'but going',\n","       'but gojoe', 'but gold', 'but gone', 'but gonna', 'but goo',\n","       'but good', 'but goodie', 'but goodness', 'but goofy',\n","       'but gordon', 'but gorehounds', 'but gossip', 'but got',\n","       'but gotta', 'but gottowt', 'but government', 'but govinda',\n","       'but gq', 'but grab', 'but graceful', 'but gracie',\n","       'but gradually', 'but graham', 'but grand', 'but granger',\n","       'but granted', 'but graphic', 'but graphically', 'but grasping',\n","       'but grateful', 'but gratuitous', 'but great', 'but greatly',\n","       'but greene', 'but greg', 'but grenade', 'but grew', 'but grey',\n","       'but grieco', 'but griffith', 'but grim', 'but grimmer',\n","       'but grinned', 'but grit', 'but gritty', 'but groovie',\n","       'but grossly', 'but grounded', 'but growing', 'but grown',\n","       'but grows', 'but guess', 'but guessed', 'but guessing',\n","       'but guinness', 'but gulf', 'but gump', 'but gundam',\n","       'but gunfighter', 'but gunga', 'but guts', 'but guy', 'but guys',\n","       'but gu√©tary', 'but gwenneth', 'but gyu', 'but had', 'but hadn',\n","       'but half', 'but halfway', 'but hallorann', 'but halloween',\n","       'but hamill', 'but hammerhead', 'but handful', 'but handicapped',\n","       'but handled', 'but handles', 'but handsome', 'but hang',\n","       'but happens', 'but happily', 'but happy', 'but hard',\n","       'but hardcore', 'but hardly', 'but harmless', 'but harmlessly',\n","       'but harriet', 'but harris', 'but harron', 'but harry',\n","       'but harsh', 'but hart', 'but has', 'but hasn', 'but hatched',\n","       'but hate', 'but hated', 'but hates', 'but hats', 'but hatton',\n","       'but haunting', 'but have', 'but haven', 'but having', 'but hay',\n","       'but hazel', 'but he', 'but head', 'but health', 'but hear',\n","       'but heard', 'but hearing', 'but heart', 'but heartbreaking',\n","       'but heartless', 'but heather', 'but heaton', 'but heaven',\n","       'but heavy', 'but heck', 'but heed', 'but heflin', 'but held',\n","       'but hell', 'but hello', 'but help', 'but helping', 'but henry',\n","       'but her', 'but herbert', 'but here', 'but heres', 'but hero',\n","       'but hers', 'but herschel', 'but herself', 'but hes',\n","       'but hesitated', 'but hesitates', 'but heston', 'but hey',\n","       'but heyman', 'but hid', 'but hideous', 'but hideously',\n","       'but hietala', 'but high', 'but higher', 'but highlighted',\n","       'but highly', 'but highpoint', 'but hilarious', 'but hilariously',\n","       'but him', 'but himself', 'but hindi', 'but hinglish', 'but his',\n","       'but historically', 'but history', 'but hit', 'but hitchcock',\n","       'but hitler', 'but hits', 'but hobgoblins', 'but hoffman',\n","       'but hogwash', 'but hold', 'but holders', 'but holidays',\n","       'but hollow', 'but hollywood', 'but home', 'but homer',\n","       'but honest', 'but honestly', 'but honorable', 'but hood',\n","       'but hoodlum', 'but hooked', 'but hooper', 'but hope', 'but hoped',\n","       'but hopeful', 'but hopefully', 'but hopelessly', 'but hopes',\n","       'but hoping', 'but hopkins', 'but horny', 'but horrible',\n","       'but horribly', 'but horrifying', 'but horrifyingly', 'but horror',\n","       'but horse', 'but hostel', 'but hot', 'but hotd', 'but hou',\n","       'but house', 'but how', 'but howard', 'but however', 'but hudson',\n","       'but huge', 'but hugely', 'but hugh', 'but hugsy', 'but huh',\n","       'but hulk', 'but human', 'but humane', 'but humbling',\n","       'but humbug', 'but humor', 'but humorously', 'but hundred',\n","       'but hunter', 'but hurley', 'but huuuge', 'but iberia',\n","       'but identifying', 'but idiocy', 'but idiots', 'but if',\n","       'but ignored', 'but ignores', 'but ignoring', 'but ihave',\n","       'but ii', 'but ill', 'but illiterate', 'but illogical', 'but im',\n","       'but imaginatively', 'but imagine', 'but imagines', 'but imdb',\n","       'but imho', 'but immediately', 'but immensely', 'but imo',\n","       'but impeccable', 'but impertubable', 'but impetuous',\n","       'but implausibility', 'but implausible', 'but implied',\n","       'but important', 'but impossible', 'but impresses',\n","       'but impressive', 'but impulse', 'but in', 'but inaccessible',\n","       'but inappropriately', 'but inarticulate', 'but inaudible',\n","       'but incapable', 'but include', 'but includes', 'but incompetent',\n","       'but increasingly', 'but incredibly', 'but incurred', 'but indeed',\n","       'but indian', 'but indiana', 'but individually', 'but ineffectual',\n","       'but inept', 'but inescapably', 'but inevitable', 'but inevitably',\n","       'but inexorable', 'but inexperienced', 'but inexplicably',\n","       'but infamous', 'but infects', 'but inferior', 'but infinitely',\n","       'but influential', 'but infomercials', 'but informed',\n","       'but innocent', 'but innovative', 'but inoffensive', 'but insane',\n","       'but inside', 'but insightful', 'but insipid', 'but insists',\n","       'but inspector', 'but inspiration', 'but inspirational',\n","       'but inspired', 'but inspires', 'but inspiring', 'but instantly',\n","       'but instead', 'but instilled', 'but instrument', 'but insult',\n","       'but insulting', 'but insures', 'but intellectually',\n","       'but intelligent', 'but intend', 'but intended', 'but intense',\n","       'but interesting', 'but interfere', 'but interference',\n","       'but interiors', 'but internal', 'but interviews', 'but into',\n","       'but intricately', 'but intriguing', 'but introduces',\n","       'but intrusively', 'but invest', 'but investment',\n","       'but invincibly', 'but invisible', 'but invite', 'but ireland',\n","       'but ironic', 'but ironically', 'but irresistable',\n","       'but irresistible', 'but irresponsible', 'but irritate',\n","       'but irritates', 'but irritating', 'but irritatingly', 'but is',\n","       'but isabella', 'but isabelle', 'but isbn', 'but isn', 'but isnt',\n","       'but it', 'but italian', 'but italians', 'but its',\n","       'but ittenbach', 'but iturbi', 'but ive', 'but izzard', 'but jack',\n","       'but jackie', 'but jacknife', 'but jafar', 'but jaffar',\n","       'but james', 'but jane', 'but janeway', 'but japan', 'but jared',\n","       'but jason', 'but jaw', 'but jaws', 'but jean', 'but jeans',\n","       'but jed', 'but jeez', 'but jennifer', 'but jenny', 'but jeremy',\n","       'but jermy', 'but jerry', 'but jesminder', 'but jessica', 'but jg',\n","       'but jill', 'but jim', 'but jimmy', 'but jindabyne', 'but jj',\n","       'but joan', 'but joanna', 'but job', 'but jodie', 'but joe',\n","       'but joey', 'but johanna', 'but johansson', 'but john',\n","       'but johnny', 'but join', 'but joke', 'but jon', 'but jong',\n","       'but jorja', 'but juan', 'but judd', 'but judges', 'but judging',\n","       'but juice', 'but jules', 'but julian', 'but julie', 'but just',\n","       'but justin', 'but kamalini', 'but kangaroo', 'but karl',\n","       'but karyn', 'but kate', 'but kathleen', 'but kazan', 'but kazuo',\n","       'but keath', 'but keaton', 'but keep', 'but keeping', 'but keeps',\n","       'but kelly', 'but ken', 'but kenneth', 'but kept', 'but kermit',\n","       'but kern', 'but kevin', 'but kicked', 'but kicking', 'but kid',\n","       'but kids', 'but kiki', 'but kill', 'but killed', 'but killer',\n","       'but killing', 'but kim', 'but kind', 'but kinda', 'but kindly',\n","       'but king', 'but kinski', 'but kirk', 'but kirkland',\n","       'but kirsten', 'but kline', 'but knew', 'but knick', 'but knock',\n","       'but know', 'but knowing', 'but known', 'but knows', 'but knox',\n","       'but kober', 'but kriemhild', 'but krishna', 'but krista',\n","       'but kudos', 'but kung', 'but kureishi', 'but kurt', 'but kyle',\n","       'but kyoto', 'but la', 'but lack', 'but lacked', 'but lacking',\n","       'but lackluster', 'but lacks', 'but lady', 'but lafitte',\n","       'but lagge', 'but lagomorph', 'but lake', 'but lambert',\n","       'but lambs', 'but lame', 'but lamour', 'but lang', 'but language',\n","       'but languorous', 'but large', 'but largely', 'but larger',\n","       'but larry', 'but last', 'but lasted', 'but lasting', 'but lately',\n","       'but latently', 'but later', 'but laugh', 'but laughable',\n","       'but laughed', 'but laughing', 'but laughs', 'but laurie',\n","       'but lawler', 'but laziness', 'but leachman', 'but leading',\n","       'but leans', 'but learn', 'but learned', 'but learns', 'but least',\n","       'but leave', 'but leaves', 'but leaving', 'but lecter', 'but led',\n","       'but lee', 'but left', 'but legend', 'but legitimate', 'but leila',\n","       'but lena', 'but leno', 'but leonora', 'but leslie', 'but less',\n","       'but lesser', 'but lest', 'but lester', 'but let', 'but lethal',\n","       'but lethargic', 'but lets', 'but letting', 'but level',\n","       'but lewis', 'but liam', 'but lie', 'but lies', 'but life',\n","       'but lifted', 'but light', 'but lightness', 'but likable',\n","       'but like', 'but likeable', 'but liked', 'but likes', 'but lilith',\n","       'but lilly', 'but lily', 'but limited', 'but lincoln',\n","       'but lingers', 'but lip', 'but list', 'but listen',\n","       'but listening', 'but literally', 'but lithgow', 'but little',\n","       'but liu', 'but live', 'but lived', 'but lives', 'but living',\n","       'but ll', 'but lloyd', 'but lo', 'but loads', 'but local',\n","       'but logical', 'but lois', 'but lonely', 'but long', 'but longed',\n","       'but longs', 'but look', 'but lookalike', 'but looked',\n","       'but looking', 'but looks', 'but loon', 'but loonatics',\n","       'but loose', 'but looses', 'but lopez', 'but lord', 'but lori',\n","       'but loser', 'but loses', 'but lost', 'but lot', 'but lots',\n","       'but lou', 'but lousy', 'but lovable', 'but love', 'but loveable',\n","       'but loved', 'but lovely', 'but loves', 'but loving', 'but low',\n","       'but lowell', 'but lower', 'but loyal', 'but lubitsch', 'but luc',\n","       'but luckily', 'but lucky', 'but lucy', 'but ludicrously',\n","       'but luger', 'but lugosi', 'but lukas', 'but lumet',\n","       'but lunacies', 'but lupe', 'but luridly', 'but lurking',\n","       'but lyricist', 'but mac', 'but mace', 'but macgraw', 'but mad',\n","       'but maddy', 'but made', 'but madras', 'but maggie',\n","       'but magically', 'but magick', 'but magnificent', 'but magnolia',\n","       'but mainly', 'but mainstream', 'but maintained', 'but major',\n","       'but majority', 'but make', 'but makes', 'but making',\n","       'but malefique', 'but malle', 'but malones', 'but malta',\n","       'but man', 'but manage', 'but managed', 'but manages',\n","       'but managing', 'but manfred', 'but mango', 'but mannequin',\n","       'but mansfield', 'but manufactured', 'but many', 'but margaux',\n","       'but marginalize', 'but marginally', 'but maria', 'but marie',\n","       'but mario', 'but mark', 'but married', 'but mars', 'but marsden',\n","       'but martin', 'but marty', 'but marvel', 'but marveled',\n","       'but marvellously', 'but mary', 'but marylee', 'but mascara',\n","       'but master', 'but masterpiece', 'but masturbation', 'but matador',\n","       'but matarazzo', 'but matin√©e', 'but matt', 'but mature',\n","       'but maugham', 'but mawkish', 'but max', 'but may', 'but maybe',\n","       'but mcinnes', 'but me', 'but meadow', 'but mean', 'but meanders',\n","       'but meaner', 'but meaning', 'but meaningful', 'but meaningless',\n","       'but meant', 'but meanwhile', 'but mechanical', 'but meddling',\n","       'but mediocre', 'but medusin', 'but meets', 'but meg', 'but meh',\n","       'but melodrama', 'but melt', 'but melvyn', 'but memoirs',\n","       'but memorable', 'but men', 'but mercifully', 'but mercurio',\n","       'but mercy', 'but mere', 'but merely', 'but merging',\n","       'but meringue', 'but meryl', 'but mess', 'but messy', 'but mexico',\n","       'but mgm', 'but miami', 'but michael', 'but michaels',\n","       'but michell', 'but mid', 'but might', 'but mighty', 'but mike',\n","       'but mild', 'but mildred', 'but miles', 'but million', 'but milo',\n","       'but mind', 'but mindless'], dtype=object)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["vectorizer_bigrams.get_feature_names_out()[219000:220000]"]},{"cell_type":"markdown","id":"99910f20","metadata":{"id":"99910f20"},"source":["### Depenalize short documents and penalize non informative words (TF-IDF)"]},{"cell_type":"markdown","id":"f1b89b74","metadata":{"id":"f1b89b74"},"source":["Due to their length, longer documents will have higher average count values than shorter documents when using BoW. To correct this, you can use TF-IDF technique to generate Term Frequencies features. To do that you have to divide the occurrences of each word in by the total number of words and give less weight to frequently occuring words in the whole corpus (TF-IDF)"]},{"cell_type":"code","execution_count":null,"id":"4cf3c7ce","metadata":{"id":"4cf3c7ce","outputId":"6fa7004b-189d-41e2-da91-6fe0c05c7d01"},"outputs":[{"data":{"text/plain":["(25000, 1513494)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","\n","transformer = TfidfTransformer(use_idf=False).fit(X_train_counts_big) #use_idf=False if you don't want to downscale frequently occuring words\n","X_train_tf = transformer.transform(X_train_counts_big)\n","X_train_tf.shape"]},{"cell_type":"code","execution_count":null,"id":"7589c178","metadata":{"id":"7589c178","outputId":"f1955554-c17d-4b0b-b000-b9af4a4b89a5"},"outputs":[{"data":{"text/plain":["<25000x1513494 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 8763731 stored elements in Compressed Sparse Row format>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["X_train_tf"]},{"cell_type":"markdown","id":"202e2d08","metadata":{"id":"202e2d08"},"source":["**You can actually use directly TfidfVectorizer on your text data if you don't want to use BoW first** https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"]},{"cell_type":"code","execution_count":null,"id":"d266cfa3","metadata":{"id":"d266cfa3"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words={'english'})\n","X = vectorizer.fit_transform(df_train.text)"]},{"cell_type":"code","execution_count":null,"id":"ded09a51","metadata":{"id":"ded09a51","outputId":"7319a199-c5f9-4da6-dedc-59b403fd040e"},"outputs":[{"data":{"text/plain":["(25000, 1513494)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["X.shape"]},{"cell_type":"markdown","id":"e9282019","metadata":{"id":"e9282019"},"source":["### Use surrounding words information with word embeddings"]},{"cell_type":"markdown","id":"b0ee048a","metadata":{"id":"b0ee048a"},"source":["Word embeddings are real-valued vector that encodes the meaning of words. Hence, close words in the vector space are expected to have similar meaning."]},{"cell_type":"markdown","id":"c9667f16","metadata":{"id":"c9667f16"},"source":["We will use SpaCy library to load english vectors and use them for preprocessing. We are doing Transfer Learning here as we intend to use knowledge from the loaded embeddings, we don't directly train them. \n","\n","By the way you can follow this Gensim tutorial to train Contiguous Bag Of Words word embeddings or Skig-gram model https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html. You'll then be able to load and use them as input for different tasks."]},{"cell_type":"code","execution_count":null,"id":"c8fc77b4","metadata":{"id":"c8fc77b4"},"outputs":[],"source":["import spacy"]},{"cell_type":"code","execution_count":null,"id":"0e38ff4c","metadata":{"scrolled":true,"id":"0e38ff4c"},"outputs":[],"source":["# !python -m spacy download en_core_web_lg"]},{"cell_type":"code","execution_count":null,"id":"9158a2f9","metadata":{"id":"9158a2f9"},"outputs":[],"source":["nlp = spacy.load(\"en_core_web_lg\")"]},{"cell_type":"markdown","id":"db4ab393","metadata":{"id":"db4ab393"},"source":["Get a vector for one sentence of our corpus"]},{"cell_type":"code","execution_count":null,"id":"d4197d2a","metadata":{"id":"d4197d2a","outputId":"aee73725-62ef-41d0-d3cd-a3a60e414f62"},"outputs":[{"data":{"text/plain":["'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967'"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["df_train.text[0].split('.')[0]"]},{"cell_type":"code","execution_count":null,"id":"61443779","metadata":{"id":"61443779"},"outputs":[],"source":["sentences = nlp(\"I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967\").vector"]},{"cell_type":"code","execution_count":null,"id":"d84dee61","metadata":{"id":"d84dee61","outputId":"4b4550b3-817e-45fd-ebaa-619802ab051a"},"outputs":[{"data":{"text/plain":["array([-1.7862697 , -0.1537471 , -1.7477717 , -0.84836197,  2.6528325 ,\n","       -0.11192326,  1.2173842 ,  4.6282744 , -1.4731263 ,  0.2748569 ,\n","        5.231145  ,  0.8304838 , -2.93644   ,  1.0369856 ,  0.6851696 ,\n","       -0.18296947,  0.7799292 , -0.09544741, -1.3605273 , -0.52190006,\n","        0.6274715 ,  1.0204049 , -1.4234464 , -0.95718956, -0.1094735 ,\n","       -2.088362  , -3.2423391 , -0.31368458, -1.461495  ,  1.7029223 ,\n","       -0.09860282,  0.14272071, -0.83620113, -0.80203307, -1.7206116 ,\n","       -1.407673  , -1.2266843 ,  1.8119164 ,  1.402984  ,  0.32291126,\n","       -0.7243057 ,  0.5358169 ,  0.47586462,  0.3491549 ,  0.28934523,\n","        0.80980694, -1.67975   , -2.2580495 , -0.9369608 ,  1.4111634 ,\n","       -1.0360786 ,  1.0836804 , -0.4873039 , -2.703148  , -1.0899733 ,\n","        0.15592119,  1.4464921 ,  0.46340114,  0.36513934,  1.5527877 ,\n","        0.9013542 , -0.6744266 ,  0.07842607,  0.8165169 , -0.11249188,\n","        0.7271983 , -3.0049589 , -1.209582  ,  1.4390137 ,  3.1445632 ,\n","       -0.22312695,  0.24818271, -0.96565   , -0.79353243,  1.1271677 ,\n","        0.8501842 , -2.547733  ,  0.7979671 , -2.555913  , -1.1261244 ,\n","       -3.7193778 ,  0.89348036,  3.3570595 , -1.7230129 ,  2.7162242 ,\n","       -0.71779215, -2.3156235 , -2.0305178 , -0.1159066 , -1.498024  ,\n","       -2.685435  ,  1.3178355 ,  3.2379935 , -4.0483236 ,  0.97531134,\n","       -0.38519767,  3.1112356 , -1.1434064 ,  0.65683144,  0.03117086,\n","        0.6710379 ,  2.2907994 ,  1.5412657 ,  1.9622375 , -1.0851096 ,\n","        2.8167806 , -0.9266683 , -1.0282369 , -1.0611405 , -3.0236895 ,\n","        0.8637918 , -0.5256569 ,  0.9808232 ,  0.4540312 ,  0.94740444,\n","        0.7203639 , -0.94005626, -0.2553238 , -0.47564486, -0.56790805,\n","       -0.2356425 , -2.6208203 , -1.1085801 ,  2.6546593 ,  0.09456277,\n","       -1.008239  ,  1.8896223 , -2.749458  ,  1.2834389 , -1.4479522 ,\n","       -2.5870588 ,  0.9661293 ,  2.6017606 , -1.1324743 , -0.13069054,\n","        0.44750044, -1.3632227 , -1.4448344 ,  3.483885  , -2.7401786 ,\n","       -1.9725446 , -0.2600973 ,  0.55854803,  0.5742357 , -0.1512507 ,\n","        1.2201962 , -2.7074087 , -0.1658281 ,  0.49495083, -0.08544044,\n","       -0.09319932,  1.5826868 , -0.09291704,  0.88743085, -0.84078306,\n","        0.928703  ,  2.609554  , -0.38584077, -0.8306253 , -0.89942014,\n","       -0.7764276 , -3.8311918 , -1.071741  ,  1.2063857 , -2.2306142 ,\n","       -0.9704806 , -1.5728633 ,  0.5236223 , -0.97823614,  1.7794175 ,\n","        0.02214623,  0.23772462,  1.1145347 ,  0.90334165,  2.063296  ,\n","       -1.424017  , -1.6396558 ,  0.05389871, -0.70536613, -0.40258121,\n","       -1.9462374 , -0.09151697,  2.5382414 , -0.6065883 , -0.74469846,\n","        0.43026277, -2.2720966 , -1.4193145 ,  1.941572  ,  0.1436198 ,\n","       -0.914615  , -0.13393475, -0.9666978 , -1.5610555 ,  0.50610375,\n","       -0.1337225 , -3.7686183 ,  0.36925304, -1.8823241 ,  2.8663542 ,\n","       -1.57628   , -1.1445707 , -0.29711112, -3.2603407 ,  2.5092218 ,\n","        0.53176177, -1.4449928 ,  1.9199569 , -0.8885985 , -0.27644923,\n","        0.91096914,  0.19606315, -0.78882766,  2.4802346 ,  0.29323   ,\n","        1.6775143 , -0.40527615, -0.49521732, -0.5776442 , -0.38681692,\n","       -3.5900807 ,  1.0606831 , -2.0641637 ,  1.3914803 , -2.195021  ,\n","       -2.6170707 ,  0.943129  ,  3.038208  ,  0.5245669 , -0.23951991,\n","        1.0928725 , -1.8101398 ,  0.55189157,  1.5667648 ,  0.72541517,\n","        2.494279  , -1.7742406 ,  2.7270055 , -1.002171  , -0.38866913,\n","       -2.6424189 , -0.29350626,  0.6885911 , -0.5800216 , -0.54235923,\n","       -1.0998126 , -0.6159006 ,  0.41961113,  1.4423726 ,  2.124209  ,\n","        1.2464827 , -0.9606439 , -6.255935  ,  1.1021616 , -0.9641766 ,\n","       -1.7075425 ,  1.6817691 ,  0.29624158, -1.6482276 ,  1.6370168 ,\n","        0.8272963 ,  5.1390467 ,  2.4335501 ,  1.7714899 ,  2.0132918 ,\n","       -0.683227  ,  0.51539576,  2.50942   , -3.6668894 ,  0.19154264,\n","        0.45082882, -1.4753473 , -1.7120059 , -1.7481939 , -0.3571346 ,\n","       -0.7824792 ,  1.2733275 , -1.376854  , -0.11928243,  0.9289323 ,\n","       -0.49683374, -0.6372396 ,  1.7284472 ,  0.6564605 ,  3.5376184 ,\n","       -1.4524318 , -0.24855536,  1.0822726 , -1.8125037 ,  1.0304027 ,\n","        1.8622652 ,  0.28171805,  0.2598948 , -0.44091064, -1.7731446 ,\n","        1.279085  ,  1.3375062 , -1.3507868 , -3.2888174 ,  1.0650253 ],\n","      dtype=float32)"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["sentences"]},{"cell_type":"markdown","id":"e1d67faa","metadata":{"id":"e1d67faa"},"source":["You can run the pipeline to create vectors for the whole corpus and just have to reshape it to create a feature matrix to use as an input for a Scikit-Learn model."]},{"cell_type":"code","execution_count":null,"id":"cd0d80c0","metadata":{"id":"cd0d80c0"},"outputs":[],"source":["import numpy as np\n","\n","# data_preprocessed = [nlp(df_train.text).vector.reshape(1,-1) for doc in corpus]\n","# feature_matrix = np.concatenate(_preprocessed)"]},{"cell_type":"markdown","id":"67decc97","metadata":{"id":"67decc97"},"source":["### Gather more context with Transformers-based architectures"]},{"cell_type":"markdown","id":"63621551","metadata":{"id":"63621551"},"source":["You gather more context with models use Transformers architecture and attention-based mechanism. I highly recommend you to read the following articles before continuing running the code to know more about this if you are not familiar with NLP.\n","\n","https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/\n","\n","https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n","\n","https://jalammar.github.io/illustrated-transformer/\n","\n","But in short, you have to know that attention allows the model to focus on the relevant parts of the input sequence, and that Transformers-based model architectures leverage those capabilities using positional encoders followed by attention mechanism, \"mapping\" how each element is linked to the others in the sequence and executing parallel queries (multi-headed attention)."]},{"cell_type":"markdown","id":"e6878f8e","metadata":{"id":"e6878f8e"},"source":["We will now go back to HuggingFace library to work on our dataset with a suitable tokenizer for BERT model which is the most-popular Transformer-based model."]},{"cell_type":"code","execution_count":null,"id":"3ec2a500","metadata":{"id":"3ec2a500"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"]},{"cell_type":"markdown","id":"70a84151","metadata":{"id":"70a84151"},"source":["We can see the following model's inputs :"]},{"cell_type":"code","execution_count":null,"id":"704a37a1","metadata":{"id":"704a37a1","outputId":"5d7b7670-2180-45f5-95d4-277819461826"},"outputs":[{"data":{"text/plain":["{'input_ids': [101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer(dataset['train'][0][\"text\"])"]},{"cell_type":"markdown","id":"5f367a5a","metadata":{"id":"5f367a5a"},"source":["We will tokenize the whole dataset :"]},{"cell_type":"code","execution_count":null,"id":"15ee79d5","metadata":{"id":"15ee79d5","outputId":"9c79aec6-020c-43a7-c963-e17161a18c20"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:05<00:00,  4.67ba/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:05<00:00,  4.27ba/s]\n"]}],"source":["def tokenize(dataset):\n","    return tokenizer(dataset[\"text\"], truncation=True)\n","\n","tokenized_dataset_train = dataset['train'].map(tokenize, batched=True)\n","tokenized_dataset_test = dataset['test'].map(tokenize, batched=True)"]},{"cell_type":"markdown","id":"0e507f36","metadata":{"id":"0e507f36"},"source":["Before training a model last step is setting the dataset type according to the Deep Learning framework you'll use (either TensorFlow or PyTorch)"]},{"cell_type":"code","execution_count":null,"id":"0a33106d","metadata":{"id":"0a33106d"},"outputs":[],"source":["#PyTorch\n","#tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n","\n","#TF\n","\n","# from transformers import DataCollatorWithPadding\n","\n","# data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n","# tf_dataset = tokenized_dataset.to_tf_dataset(\n","#     columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],\n","#     label_cols=[\"labels\"],\n","#     batch_size=2,\n","#     collate_fn=data_collator,\n","#     shuffle=True\n","# )"]},{"cell_type":"markdown","id":"aa39a008","metadata":{"id":"aa39a008"},"source":["**Credits to original tutorials from Scikit-Learn and HuggingFace :**\n","\n","https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n","\n","https://huggingface.co/docs/datasets/use_dataset"]},{"cell_type":"markdown","id":"566bce58","metadata":{"id":"566bce58"},"source":["# Train your sentiment analysis model"]},{"cell_type":"markdown","id":"19ea48b5","metadata":{"id":"19ea48b5"},"source":["### Use a classical ML algorithm"]},{"cell_type":"markdown","id":"14968d11","metadata":{"id":"14968d11"},"source":["We will first use a Scikit-Learn pipeline to train a classical SVM classifier and use GridSearch Cross Validation. As we already preprocessed the data we will not run the vectorization part but you can see that every step of your workflow can be put into a pipeline, which is more convenient."]},{"cell_type":"code","execution_count":null,"id":"b52b7c1a","metadata":{"id":"b52b7c1a"},"outputs":[],"source":["from sklearn.svm import LinearSVC\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import GridSearchCV\n","from sklearn import metrics\n","\n","parameters =  {}\n","# {\n","#     'vect__ngram_range': [(1, 1), (1, 2)],\n","# }\n","pipeline = Pipeline([\n","#     ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),\n","    ('clf', LinearSVC(C=1000)),\n","])"]},{"cell_type":"code","execution_count":null,"id":"2992667b","metadata":{"id":"2992667b"},"outputs":[],"source":["grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n","grid_search.fit(X, df_train.label)"]},{"cell_type":"markdown","id":"f4afce5d","metadata":{"id":"f4afce5d"},"source":["If you provide many parameters to search over, you can look at their specific results with the following code (here we didn't specify any params)."]},{"cell_type":"code","execution_count":null,"id":"08d3b978","metadata":{"id":"08d3b978","outputId":"4498abba-3f4c-4c3c-c66a-ee85b45a94f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["0 params - {}; mean - 0.88; std - 0.01\n"]}],"source":["n_candidates = len(grid_search.cv_results_['params'])\n","for i in range(n_candidates):\n","    print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n","             % (grid_search.cv_results_['params'][i],\n","                grid_search.cv_results_['mean_test_score'][i],\n","                grid_search.cv_results_['std_test_score'][i]))"]},{"cell_type":"markdown","id":"c914fb01","metadata":{"id":"c914fb01"},"source":["Let's look at classification report and confusion matrix :\n","\n","To brush up on these topics please see :\n","\n","https://scikit-learn.org/stable/modules/model_evaluation.html"]},{"cell_type":"code","execution_count":null,"id":"1c78c8d9","metadata":{"id":"1c78c8d9"},"outputs":[],"source":["X_test = vectorizer.transform(df_test.text)"]},{"cell_type":"code","execution_count":null,"id":"7320de34","metadata":{"id":"7320de34"},"outputs":[],"source":["y_predicted = grid_search.predict(X_test)"]},{"cell_type":"code","execution_count":null,"id":"bbac838e","metadata":{"id":"bbac838e","outputId":"3ab8ddd7-378c-489f-ee77-b5eae3228b03"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.90      0.90      0.90     12500\n","           1       0.90      0.90      0.90     12500\n","\n","    accuracy                           0.90     25000\n","   macro avg       0.90      0.90      0.90     25000\n","weighted avg       0.90      0.90      0.90     25000\n","\n"]}],"source":["print(metrics.classification_report(df_test.label, y_predicted))"]},{"cell_type":"code","execution_count":null,"id":"06b063e6","metadata":{"id":"06b063e6","outputId":"517fbf87-91d5-4e9b-8bbe-cd2d3002ed8a"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[11298  1202]\n"," [ 1237 11263]]\n"]}],"source":["confusion_matrix = metrics.confusion_matrix(df_test.label, y_predicted)\n","print(confusion_matrix)"]},{"cell_type":"markdown","id":"dae5e301","metadata":{"id":"dae5e301"},"source":["### Use BERT model"]},{"cell_type":"markdown","id":"503b534a","metadata":{"id":"503b534a"},"source":["Here we will use a HuggingFace pipeline for sentiment analysis. You can run the code up to training if your configuration isn't suitable for finetuning a large model. \n","\n","**You might need to had an accelerator to Google's VM by going to Runtime>Change Runtime Type and selection a GPU if you want to fine-tune the model. The notebook will be reinitialized so don't forget to re-run previous cell using HuggingFace library code**"]},{"cell_type":"code","execution_count":null,"id":"5cb332a6","metadata":{"id":"5cb332a6"},"outputs":[],"source":["from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer"]},{"cell_type":"markdown","id":"d56a0b8b","metadata":{"id":"d56a0b8b"},"source":["We'll do the fine-tuning (use BERT pretrained layers and adapt last to our sentiment analysis problem on our dataset) in PyTorch. The model can be converted to TF later. \n","\n","You select an hardware accelerator on Google Colab (Runtime > Change Runtime).\n","Check if a GPU is available :"]},{"cell_type":"code","execution_count":null,"id":"a2ebfd9a","metadata":{"id":"a2ebfd9a","outputId":"6475ba37-1da8-4c48-f6dc-a87957e8511f"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","\n","torch.cuda.is_available()"]},{"cell_type":"markdown","id":"a831b59b","metadata":{"id":"a831b59b"},"source":["I False, do not run the training it will be too long."]},{"cell_type":"markdown","id":"5c1a5c0a","metadata":{"id":"5c1a5c0a"},"source":["We already tokenized the dataset earlier. Now we will use a data collator to convert inputs to PyTorch tensors."]},{"cell_type":"code","execution_count":null,"id":"99398253","metadata":{"id":"99398253"},"outputs":[],"source":["from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"id":"aed2bf1e","metadata":{"id":"aed2bf1e","outputId":"21dd8e8e-9e9d-4118-819d-13901852a922"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoModelForSequenceClassification\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"]},{"cell_type":"code","execution_count":null,"id":"ca652064","metadata":{"id":"ca652064"},"outputs":[],"source":["from datasets import load_metric\n"," \n","def compute_metrics(eval_pred):\n","    load_accuracy = load_metric(\"accuracy\")\n","    load_f1 = load_metric(\"f1\")\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n","    f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n","    return {\"accuracy\": accuracy, \"f1\": f1}"]},{"cell_type":"code","execution_count":null,"id":"98d9395d","metadata":{"id":"98d9395d"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","\n","repo_name = \"omdena_workshop\"\n","training_args = TrainingArguments(\n","   output_dir=repo_name,\n","   learning_rate=2e-5,\n","   per_device_train_batch_size=16,\n","   per_device_eval_batch_size=16,\n","   num_train_epochs=2,\n","   weight_decay=0.01,\n","   save_strategy=\"epoch\",\n","   push_to_hub=False,\n",")\n"," \n","trainer = Trainer(\n","   model=model,\n","   args=training_args,\n","   train_dataset=tokenized_dataset_train,\n","   eval_dataset=tokenized_dataset_test,\n","   tokenizer=tokenizer,\n","   data_collator=data_collator,\n","   compute_metrics=compute_metrics,\n",")\n"]},{"cell_type":"code","execution_count":null,"id":"39d000d2","metadata":{"id":"39d000d2"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"id":"2140add7","metadata":{"id":"2140add7"},"outputs":[],"source":["trainer.evaluate()"]},{"cell_type":"markdown","source":["To \"convert\" your model to TF format do (it loads your model checkpoint into TF format) :"],"metadata":{"id":"LTioWWNL8XRf"},"id":"LTioWWNL8XRf"},{"cell_type":"code","source":["TFAutoModelForSequenceClassification.from_pretrained(\"omdena_workshop/\", from_pt=True)"],"metadata":{"id":"eb4haTX48Wm4"},"id":"eb4haTX48Wm4","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"815076f8","metadata":{"id":"815076f8"},"source":["We can use our model in a pipeline"]},{"cell_type":"code","execution_count":null,"id":"be80f464","metadata":{"id":"be80f464"},"outputs":[],"source":["# Sentiment analysis pipeline\n","pipeline = pipeline(\"sentiment-analysis\", model=\"omdena_workshop/\")"]},{"cell_type":"code","execution_count":null,"id":"a2bff076","metadata":{"id":"a2bff076"},"outputs":[],"source":["pipeline([\"This movie was a dream to watch\", \"This movie really sucks, I left before the end of the projection!\"])"]},{"cell_type":"markdown","id":"4906090c","metadata":{"id":"4906090c"},"source":["Keep in mind that BERT isn't the only convenient model out there, I took it for example but you could have used a distilled version (DistillBERT) which is faster, especially if you have inference requirements in mind. There are plenty of Transformers-powered models avaiable in [HuggingFace Hub](https://) I encourage you to visit and experiment from."]},{"cell_type":"markdown","id":"cbb3d790","metadata":{"id":"cbb3d790"},"source":["**Credits goes again to HuggingFace for tutorial hints :**\n","\n","https://huggingface.co/blog/sentiment-analysis-python"]},{"cell_type":"markdown","id":"3c1a1d88","metadata":{"id":"3c1a1d88"},"source":["Please note that you can also now use HuggingFace generated embeddings in a Scikit-Learn pipeline as stated here :\n","\n","https://huggingface.co/scikit-learn/sklearn-transformers\n","\n","https://huggingface.co/scikit-learn/skorch-text-classification"]}],"metadata":{"kernelspec":{"display_name":"omdena_workshop","language":"python","name":"omdena_workshop"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}